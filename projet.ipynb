{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os \n",
    "\n",
    "# Importations datasets\n",
    "from sklearn import datasets\n",
    "\n",
    "# Pré-modèles !\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# TRANSFORMATION DATAFRAME\n",
    "\n",
    "# Transformation de la valeur en texte\n",
    "def text_type_select(value, feature, select=0) :\n",
    "    \"\"\" Retourne un type de texte selon la sélection\n",
    "    0. <value> <feature>\n",
    "    1. <value> <unit> of <name> (avec feature = (<name>, <unit>))\n",
    "\n",
    "    Args:\n",
    "        value (float): la valeur du feature\n",
    "        feature (str): le nom de feature\n",
    "        select (int, optional): Type de texte à sélectionner. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        str: Texte retourné\n",
    "    \"\"\"\n",
    "    if select == 0 :\n",
    "        return str(value) + \" \" + str(feature)\n",
    "    if select == 1 :\n",
    "        name, unit = feature\n",
    "        return str(value) + \" \" + str(unit) + \" of \" + str(name)\n",
    "    \n",
    "# Transformation d'une ligne en texte\n",
    "def df_row_to_text(Xi:pd.DataFrame, yi:pd.DataFrame, feature_names, subject_name = \"subject\", has_unit=False) :\n",
    "    \"\"\" Pour une ligne d'un DataFrame, génère un texte expliquant la ligne\n",
    "\n",
    "    Args:\n",
    "        `df` (pd.DataFrame): Le DataFrame\n",
    "        `row_num` (int): La ligne du DataFrame dont on veut générer la description en texte\n",
    "        `label_num` (int, optional): La colonne contenant le label. Defaults to -1 (la dernière colonne).\n",
    "        `subject_name` (str, optional): Le nom du sujet pour l'affichage. Defaults to \"subject\".\n",
    "        `has_unit` (bool, optional): _description_. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        str: Le texte généré\n",
    "    \"\"\"\n",
    "\n",
    "    # features name - unit\n",
    "    if has_unit :\n",
    "        try :\n",
    "            regex_unit_pattern = r'([\\w ]+)\\s+\\((\\w+)\\)$'\n",
    "            feature_names = [re.search(regex_unit_pattern, feature).groups() for feature in feature_names] \n",
    "            has_unit_vector = [True if len(tuple) == 2 else False for tuple in feature_names]\n",
    "        except :\n",
    "            has_unit_vector = [False for _ in feature_names]\n",
    "    else :\n",
    "        has_unit_vector = [False for _ in feature_names]\n",
    "\n",
    "    # generate text type\n",
    "    value_feature_text_list = [text_type_select(value, feature, select=1) if has_unit_vector[i] \\\n",
    "                               else text_type_select(value, feature, select=0) \\\n",
    "                               for i, (value, feature) in enumerate(zip(Xi, feature_names))]\n",
    "\n",
    "    # generate text\n",
    "    text = \"The \" + subject_name + \" with \"\n",
    "    for vf in value_feature_text_list[:-1] :\n",
    "        text += vf + \", \"\n",
    "    text = text[:-2]\n",
    "    text += \" and \" + value_feature_text_list[-1]\n",
    "    text += \" is \" + str(yi) # a / an \n",
    "\n",
    "    return text\n",
    "\n",
    "# Liste de textes\n",
    "def df_texts_list(X:pd.DataFrame, y:pd.DataFrame, **kwargs) :\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): _description_\n",
    "        label_num (int, optional): _description_. Defaults to -1.\n",
    "        subject_name (str, optional): _description_. Defaults to \"subject\".\n",
    "        has_unit (bool, optional): _description_. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    return [df_row_to_text(X.iloc[i], y.iloc[i], X.columns, **kwargs) for i in range(len(X))]\n",
    "\n",
    "# DataFrame Texte (Main)\n",
    "def data_to_df_text(data, **kwargs) :\n",
    "    if 'feature_names' in data :\n",
    "        X = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
    "    else :\n",
    "        X = pd.DataFrame(data['data'])\n",
    "    y = pd.Series(data['target'])\n",
    "    if 'target_names' in data :\n",
    "        y = y.map({i:label for i, label in enumerate(data['target_names'])})\n",
    "    return pd.Series(df_texts_list(X, y, **kwargs))\n",
    "\n",
    "# DataFrame Display (Main)\n",
    "def display_df(df:pd.DataFrame) :\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    print(df)\n",
    "    pd.reset_option('display.max_rows')\n",
    "    pd.reset_option('display.max_colwidth')\n",
    "\n",
    "# -------------------------------------------\n",
    "# TRAINING\n",
    "\n",
    "# Training (Main)\n",
    "def train_clf(clf, X_train, y_train) :\n",
    "    clf.fit(X_train, y_train) \n",
    "    print(clf.predict(X_train))\n",
    "\n",
    "# -------------------------------------------\n",
    "# TRAITEMENT\n",
    "\n",
    "# Récupérer la liste de features et le nom de label de la question\n",
    "def question_to_list(q) :\n",
    "    pattern = r'be (.*) \\?'\n",
    "    match = re.search(pattern, q)\n",
    "    a_label = match.group(1)\n",
    "\n",
    "    pattern = r'have (.*), what'\n",
    "    match = re.search(pattern, q.strip())\n",
    "    a_features = match.group(1)\n",
    "    a_list_features = a_features.split(',')\n",
    "    a_list_features = [feature.strip() for feature in a_list_features]\n",
    "\n",
    "    return a_list_features, a_label\n",
    "\n",
    "# Transformer en DataFrame pour faire passer dans le calcul de prédiction\n",
    "def question_to_df(q) :\n",
    "    a_list_features, _ = question_to_list(q)\n",
    "    a_list_features_split = [feature.split('=') for feature in a_list_features]\n",
    "    a_features_names = [feature[0] for feature in a_list_features_split]\n",
    "    try : \n",
    "        a_features_values = [float(feature[1]) for feature in a_list_features_split]\n",
    "    except :\n",
    "        a_features_values = [feature[1] for feature in a_list_features_split]\n",
    "    return pd.DataFrame(data=np.array([a_features_values]), columns=a_features_names)\n",
    "\n",
    "# Prédiction de la réponse\n",
    "def q_df_to_answer(clf, df) :\n",
    "    return clf.predict(df)[0]\n",
    "\n",
    "# Prédiction de la réponse\n",
    "def answer_to_text(q, a) :\n",
    "    _, a_label = question_to_list(q)\n",
    "    pattern = r'what .* \\?'\n",
    "    qa = re.sub(pattern, '', q.strip())\n",
    "    return qa.strip() + \" \" + a_label + \" is \" + str(a)\n",
    "\n",
    "# Réponse du programme à partir de la question\n",
    "def traitement_question(clf, q) :\n",
    "    q_df = question_to_df(q)\n",
    "    a = q_df_to_answer(clf, q_df)\n",
    "    return answer_to_text(q, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning du modèle\n",
    "def train_premodel(model_name=\"gpt2\", dataset_path=\"./iris_dataset.txt\", save_path=\"./fine-tuned-gpt2\") :\n",
    "\n",
    "    # Load pre-trained GPT-2 model and tokenizer\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    dataset = TextDataset(tokenizer=tokenizer, file_path=dataset_path, block_size=128)\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=save_path,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=4,\n",
    "        save_steps=10_000,\n",
    "        save_total_limit=2,\n",
    "    )\n",
    "\n",
    "    # Create Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    model.save_pretrained(save_path)\n",
    "\n",
    "# Récupérer le model et le tokenizer\n",
    "def get_model_tokenizer(model_path=\"./fine-tuned-gpt2\", tokenizer_path=\"gpt2\") :\n",
    "    # Load fine-tuned GPT-2 model and tokenizer\n",
    "    model_path = model_path\n",
    "    tokenizer_path = tokenizer_path\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path, local_files_only=True)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Tester le modèle\n",
    "def test_premodel(model, tokenizer, prompt=\"The flower with\", max_length=100, temperature=1, top_k=1) :\n",
    "\n",
    "    # Generate text samples\n",
    "    prompt = \"The flower with\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, temperature=temperature, top_k=top_k) # top_k=50\n",
    "\n",
    "    # Decode generated output\n",
    "    generated_texts = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "\n",
    "    # Print generated texts\n",
    "    for i, text in enumerate(generated_texts):\n",
    "        print(f\"Generated Text {i+1}: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Préparation du Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module']\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# 1. Chargement du Dataset Iris\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "print([key for key in iris])\n",
    "print(iris['feature_names'])\n",
    "print(iris['target_names'])\n",
    "\n",
    "X = pd.DataFrame(iris['data'], columns=iris['feature_names'])\n",
    "y = pd.Series(iris['target']).map({i:label for i, label in enumerate(iris['target_names'])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fabrication du Dataset de textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The flower with 6.7 sepal length (cm), 3.0 sepal width (cm), 5.2 petal length (cm) and 2.3 petal width (cm) is 2\n",
      "The flower with 6.7 cm of sepal length, 3.0 cm of sepal width, 5.2 cm of petal length and 2.3 cm of petal width is 2\n"
     ]
    }
   ],
   "source": [
    "# 2.1. Transformation d'une ligne\n",
    "\n",
    "row = 145\n",
    "print(df_row_to_text(iris['data'][row], iris['target'][row], iris['feature_names'], subject_name=\"flower\", has_unit=False))\n",
    "print(df_row_to_text(iris['data'][row], iris['target'][row], iris['feature_names'], subject_name=\"flower\", has_unit=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The flower with 5.1 cm of sepal length, 3.5 cm of sepal width, 1.4 cm of petal length and 0.2 cm of petal width is setosa\n",
      "The flower with 4.9 cm of sepal length, 3.0 cm of sepal width, 1.4 cm of petal length and 0.2 cm of petal width is setosa\n",
      "The flower with 4.7 cm of sepal length, 3.2 cm of sepal width, 1.3 cm of petal length and 0.2 cm of petal width is setosa\n",
      "The flower with 4.6 cm of sepal length, 3.1 cm of sepal width, 1.5 cm of petal length and 0.2 cm of petal width is setosa\n",
      "The flower with 5.0 cm of sepal length, 3.6 cm of sepal width, 1.4 cm of petal length and 0.2 cm of petal width is setosa\n",
      "The flower with 6.7 cm of sepal length, 3.0 cm of sepal width, 5.2 cm of petal length and 2.3 cm of petal width is virginica\n",
      "The flower with 6.3 cm of sepal length, 2.5 cm of sepal width, 5.0 cm of petal length and 1.9 cm of petal width is virginica\n",
      "The flower with 6.5 cm of sepal length, 3.0 cm of sepal width, 5.2 cm of petal length and 2.0 cm of petal width is virginica\n",
      "The flower with 6.2 cm of sepal length, 3.4 cm of sepal width, 5.4 cm of petal length and 2.3 cm of petal width is virginica\n",
      "The flower with 5.9 cm of sepal length, 3.0 cm of sepal width, 5.1 cm of petal length and 1.8 cm of petal width is virginica\n"
     ]
    }
   ],
   "source": [
    "# 2.2. Conversion des samples en texte\n",
    "\n",
    "iris_text_list = df_texts_list(X, y, subject_name=\"flower\", has_unit=True)\n",
    "\n",
    "for text in iris_text_list[:5] + iris_text_list[-5:] :\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    The flower with 5.1 cm of sepal length, 3.5 cm...\n",
       "1    The flower with 4.9 cm of sepal length, 3.0 cm...\n",
       "2    The flower with 4.7 cm of sepal length, 3.2 cm...\n",
       "3    The flower with 4.6 cm of sepal length, 3.1 cm...\n",
       "4    The flower with 5.0 cm of sepal length, 3.6 cm...\n",
       "5    The flower with 5.4 cm of sepal length, 3.9 cm...\n",
       "6    The flower with 4.6 cm of sepal length, 3.4 cm...\n",
       "7    The flower with 5.0 cm of sepal length, 3.4 cm...\n",
       "8    The flower with 4.4 cm of sepal length, 2.9 cm...\n",
       "9    The flower with 4.9 cm of sepal length, 3.1 cm...\n",
       "dtype: object"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.3. Création d'un nouveau DF\n",
    "\n",
    "iris_text_df = data_to_df_text(iris, subject_name=\"flower\", has_unit=True)\n",
    "iris_text_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        The number with 0.0 pixel_0_0, 0.0 pixel_0_1, 5.0 pixel_0_2, 13.0 pixel_0_3, 9.0 pixel_0_4, 1.0 pixel_0_5, 0.0 pixel_0_6, 0.0 pixel_0_7, 0.0 pixel_1_0, 0.0 pixel_1_1, 13.0 pixel_1_2, 15.0 pixel_1_3, 10.0 pixel_1_4, 15.0 pixel_1_5, 5.0 pixel_1_6, 0.0 pixel_1_7, 0.0 pixel_2_0, 3.0 pixel_2_1, 15.0 pixel_2_2, 2.0 pixel_2_3, 0.0 pixel_2_4, 11.0 pixel_2_5, 8.0 pixel_2_6, 0.0 pixel_2_7, 0.0 pixel_3_0, 4.0 pixel_3_1, 12.0 pixel_3_2, 0.0 pixel_3_3, 0.0 pixel_3_4, 8.0 pixel_3_5, 8.0 pixel_3_6, 0.0 pixel_3_7, 0.0 pixel_4_0, 5.0 pixel_4_1, 8.0 pixel_4_2, 0.0 pixel_4_3, 0.0 pixel_4_4, 9.0 pixel_4_5, 8.0 pixel_4_6, 0.0 pixel_4_7, 0.0 pixel_5_0, 4.0 pixel_5_1, 11.0 pixel_5_2, 0.0 pixel_5_3, 1.0 pixel_5_4, 12.0 pixel_5_5, 7.0 pixel_5_6, 0.0 pixel_5_7, 0.0 pixel_6_0, 2.0 pixel_6_1, 14.0 pixel_6_2, 5.0 pixel_6_3, 10.0 pixel_6_4, 12.0 pixel_6_5, 0.0 pixel_6_6, 0.0 pixel_6_7, 0.0 pixel_7_0, 0.0 pixel_7_1, 6.0 pixel_7_2, 13.0 pixel_7_3, 10.0 pixel_7_4, 0.0 pixel_7_5, 0.0 pixel_7_6 and 0.0 pixel_7_7 is 0\n",
      "1     The number with 0.0 pixel_0_0, 0.0 pixel_0_1, 0.0 pixel_0_2, 12.0 pixel_0_3, 13.0 pixel_0_4, 5.0 pixel_0_5, 0.0 pixel_0_6, 0.0 pixel_0_7, 0.0 pixel_1_0, 0.0 pixel_1_1, 0.0 pixel_1_2, 11.0 pixel_1_3, 16.0 pixel_1_4, 9.0 pixel_1_5, 0.0 pixel_1_6, 0.0 pixel_1_7, 0.0 pixel_2_0, 0.0 pixel_2_1, 3.0 pixel_2_2, 15.0 pixel_2_3, 16.0 pixel_2_4, 6.0 pixel_2_5, 0.0 pixel_2_6, 0.0 pixel_2_7, 0.0 pixel_3_0, 7.0 pixel_3_1, 15.0 pixel_3_2, 16.0 pixel_3_3, 16.0 pixel_3_4, 2.0 pixel_3_5, 0.0 pixel_3_6, 0.0 pixel_3_7, 0.0 pixel_4_0, 0.0 pixel_4_1, 1.0 pixel_4_2, 16.0 pixel_4_3, 16.0 pixel_4_4, 3.0 pixel_4_5, 0.0 pixel_4_6, 0.0 pixel_4_7, 0.0 pixel_5_0, 0.0 pixel_5_1, 1.0 pixel_5_2, 16.0 pixel_5_3, 16.0 pixel_5_4, 6.0 pixel_5_5, 0.0 pixel_5_6, 0.0 pixel_5_7, 0.0 pixel_6_0, 0.0 pixel_6_1, 1.0 pixel_6_2, 16.0 pixel_6_3, 16.0 pixel_6_4, 6.0 pixel_6_5, 0.0 pixel_6_6, 0.0 pixel_6_7, 0.0 pixel_7_0, 0.0 pixel_7_1, 0.0 pixel_7_2, 11.0 pixel_7_3, 16.0 pixel_7_4, 10.0 pixel_7_5, 0.0 pixel_7_6 and 0.0 pixel_7_7 is 1\n",
      "2    The number with 0.0 pixel_0_0, 0.0 pixel_0_1, 0.0 pixel_0_2, 4.0 pixel_0_3, 15.0 pixel_0_4, 12.0 pixel_0_5, 0.0 pixel_0_6, 0.0 pixel_0_7, 0.0 pixel_1_0, 0.0 pixel_1_1, 3.0 pixel_1_2, 16.0 pixel_1_3, 15.0 pixel_1_4, 14.0 pixel_1_5, 0.0 pixel_1_6, 0.0 pixel_1_7, 0.0 pixel_2_0, 0.0 pixel_2_1, 8.0 pixel_2_2, 13.0 pixel_2_3, 8.0 pixel_2_4, 16.0 pixel_2_5, 0.0 pixel_2_6, 0.0 pixel_2_7, 0.0 pixel_3_0, 0.0 pixel_3_1, 1.0 pixel_3_2, 6.0 pixel_3_3, 15.0 pixel_3_4, 11.0 pixel_3_5, 0.0 pixel_3_6, 0.0 pixel_3_7, 0.0 pixel_4_0, 1.0 pixel_4_1, 8.0 pixel_4_2, 13.0 pixel_4_3, 15.0 pixel_4_4, 1.0 pixel_4_5, 0.0 pixel_4_6, 0.0 pixel_4_7, 0.0 pixel_5_0, 9.0 pixel_5_1, 16.0 pixel_5_2, 16.0 pixel_5_3, 5.0 pixel_5_4, 0.0 pixel_5_5, 0.0 pixel_5_6, 0.0 pixel_5_7, 0.0 pixel_6_0, 3.0 pixel_6_1, 13.0 pixel_6_2, 16.0 pixel_6_3, 16.0 pixel_6_4, 11.0 pixel_6_5, 5.0 pixel_6_6, 0.0 pixel_6_7, 0.0 pixel_7_0, 0.0 pixel_7_1, 0.0 pixel_7_2, 3.0 pixel_7_3, 11.0 pixel_7_4, 16.0 pixel_7_5, 9.0 pixel_7_6 and 0.0 pixel_7_7 is 2\n",
      "3         The number with 0.0 pixel_0_0, 0.0 pixel_0_1, 7.0 pixel_0_2, 15.0 pixel_0_3, 13.0 pixel_0_4, 1.0 pixel_0_5, 0.0 pixel_0_6, 0.0 pixel_0_7, 0.0 pixel_1_0, 8.0 pixel_1_1, 13.0 pixel_1_2, 6.0 pixel_1_3, 15.0 pixel_1_4, 4.0 pixel_1_5, 0.0 pixel_1_6, 0.0 pixel_1_7, 0.0 pixel_2_0, 2.0 pixel_2_1, 1.0 pixel_2_2, 13.0 pixel_2_3, 13.0 pixel_2_4, 0.0 pixel_2_5, 0.0 pixel_2_6, 0.0 pixel_2_7, 0.0 pixel_3_0, 0.0 pixel_3_1, 2.0 pixel_3_2, 15.0 pixel_3_3, 11.0 pixel_3_4, 1.0 pixel_3_5, 0.0 pixel_3_6, 0.0 pixel_3_7, 0.0 pixel_4_0, 0.0 pixel_4_1, 0.0 pixel_4_2, 1.0 pixel_4_3, 12.0 pixel_4_4, 12.0 pixel_4_5, 1.0 pixel_4_6, 0.0 pixel_4_7, 0.0 pixel_5_0, 0.0 pixel_5_1, 0.0 pixel_5_2, 0.0 pixel_5_3, 1.0 pixel_5_4, 10.0 pixel_5_5, 8.0 pixel_5_6, 0.0 pixel_5_7, 0.0 pixel_6_0, 0.0 pixel_6_1, 8.0 pixel_6_2, 4.0 pixel_6_3, 5.0 pixel_6_4, 14.0 pixel_6_5, 9.0 pixel_6_6, 0.0 pixel_6_7, 0.0 pixel_7_0, 0.0 pixel_7_1, 7.0 pixel_7_2, 13.0 pixel_7_3, 13.0 pixel_7_4, 9.0 pixel_7_5, 0.0 pixel_7_6 and 0.0 pixel_7_7 is 3\n",
      "4          The number with 0.0 pixel_0_0, 0.0 pixel_0_1, 0.0 pixel_0_2, 1.0 pixel_0_3, 11.0 pixel_0_4, 0.0 pixel_0_5, 0.0 pixel_0_6, 0.0 pixel_0_7, 0.0 pixel_1_0, 0.0 pixel_1_1, 0.0 pixel_1_2, 7.0 pixel_1_3, 8.0 pixel_1_4, 0.0 pixel_1_5, 0.0 pixel_1_6, 0.0 pixel_1_7, 0.0 pixel_2_0, 0.0 pixel_2_1, 1.0 pixel_2_2, 13.0 pixel_2_3, 6.0 pixel_2_4, 2.0 pixel_2_5, 2.0 pixel_2_6, 0.0 pixel_2_7, 0.0 pixel_3_0, 0.0 pixel_3_1, 7.0 pixel_3_2, 15.0 pixel_3_3, 0.0 pixel_3_4, 9.0 pixel_3_5, 8.0 pixel_3_6, 0.0 pixel_3_7, 0.0 pixel_4_0, 5.0 pixel_4_1, 16.0 pixel_4_2, 10.0 pixel_4_3, 0.0 pixel_4_4, 16.0 pixel_4_5, 6.0 pixel_4_6, 0.0 pixel_4_7, 0.0 pixel_5_0, 4.0 pixel_5_1, 15.0 pixel_5_2, 16.0 pixel_5_3, 13.0 pixel_5_4, 16.0 pixel_5_5, 1.0 pixel_5_6, 0.0 pixel_5_7, 0.0 pixel_6_0, 0.0 pixel_6_1, 0.0 pixel_6_2, 3.0 pixel_6_3, 15.0 pixel_6_4, 10.0 pixel_6_5, 0.0 pixel_6_6, 0.0 pixel_6_7, 0.0 pixel_7_0, 0.0 pixel_7_1, 0.0 pixel_7_2, 2.0 pixel_7_3, 16.0 pixel_7_4, 4.0 pixel_7_5, 0.0 pixel_7_6 and 0.0 pixel_7_7 is 4\n",
      "dtype: object\n",
      "0    The wine with 14.23 alcohol, 1.71 malic_acid, 2.43 ash, 15.6 alcalinity_of_ash, 127.0 magnesium, 2.8 total_phenols, 3.06 flavanoids, 0.28 nonflavanoid_phenols, 2.29 proanthocyanins, 5.64 color_intensity, 1.04 hue, 3.92 od280/od315_of_diluted_wines and 1065.0 proline is class_0\n",
      "1     The wine with 13.2 alcohol, 1.78 malic_acid, 2.14 ash, 11.2 alcalinity_of_ash, 100.0 magnesium, 2.65 total_phenols, 2.76 flavanoids, 0.26 nonflavanoid_phenols, 1.28 proanthocyanins, 4.38 color_intensity, 1.05 hue, 3.4 od280/od315_of_diluted_wines and 1050.0 proline is class_0\n",
      "2     The wine with 13.16 alcohol, 2.36 malic_acid, 2.67 ash, 18.6 alcalinity_of_ash, 101.0 magnesium, 2.8 total_phenols, 3.24 flavanoids, 0.3 nonflavanoid_phenols, 2.81 proanthocyanins, 5.68 color_intensity, 1.03 hue, 3.17 od280/od315_of_diluted_wines and 1185.0 proline is class_0\n",
      "3     The wine with 14.37 alcohol, 1.95 malic_acid, 2.5 ash, 16.8 alcalinity_of_ash, 113.0 magnesium, 3.85 total_phenols, 3.49 flavanoids, 0.24 nonflavanoid_phenols, 2.18 proanthocyanins, 7.8 color_intensity, 0.86 hue, 3.45 od280/od315_of_diluted_wines and 1480.0 proline is class_0\n",
      "4     The wine with 13.24 alcohol, 2.59 malic_acid, 2.87 ash, 21.0 alcalinity_of_ash, 118.0 magnesium, 2.8 total_phenols, 2.69 flavanoids, 0.39 nonflavanoid_phenols, 1.82 proanthocyanins, 4.32 color_intensity, 1.04 hue, 2.93 od280/od315_of_diluted_wines and 735.0 proline is class_0\n",
      "dtype: object\n",
      "0     The breast cancer type with 17.99 mean radius, 10.38 mean texture, 122.8 mean perimeter, 1001.0 mean area, 0.1184 mean smoothness, 0.2776 mean compactness, 0.3001 mean concavity, 0.1471 mean concave points, 0.2419 mean symmetry, 0.07871 mean fractal dimension, 1.095 radius error, 0.9053 texture error, 8.589 perimeter error, 153.4 area error, 0.006399 smoothness error, 0.04904 compactness error, 0.05373 concavity error, 0.01587 concave points error, 0.03003 symmetry error, 0.006193 fractal dimension error, 25.38 worst radius, 17.33 worst texture, 184.6 worst perimeter, 2019.0 worst area, 0.1622 worst smoothness, 0.6656 worst compactness, 0.7119 worst concavity, 0.2654 worst concave points, 0.4601 worst symmetry and 0.1189 worst fractal dimension is malignant\n",
      "1    The breast cancer type with 20.57 mean radius, 17.77 mean texture, 132.9 mean perimeter, 1326.0 mean area, 0.08474 mean smoothness, 0.07864 mean compactness, 0.0869 mean concavity, 0.07017 mean concave points, 0.1812 mean symmetry, 0.05667 mean fractal dimension, 0.5435 radius error, 0.7339 texture error, 3.398 perimeter error, 74.08 area error, 0.005225 smoothness error, 0.01308 compactness error, 0.0186 concavity error, 0.0134 concave points error, 0.01389 symmetry error, 0.003532 fractal dimension error, 24.99 worst radius, 23.41 worst texture, 158.8 worst perimeter, 1956.0 worst area, 0.1238 worst smoothness, 0.1866 worst compactness, 0.2416 worst concavity, 0.186 worst concave points, 0.275 worst symmetry and 0.08902 worst fractal dimension is malignant\n",
      "2      The breast cancer type with 19.69 mean radius, 21.25 mean texture, 130.0 mean perimeter, 1203.0 mean area, 0.1096 mean smoothness, 0.1599 mean compactness, 0.1974 mean concavity, 0.1279 mean concave points, 0.2069 mean symmetry, 0.05999 mean fractal dimension, 0.7456 radius error, 0.7869 texture error, 4.585 perimeter error, 94.03 area error, 0.00615 smoothness error, 0.04006 compactness error, 0.03832 concavity error, 0.02058 concave points error, 0.0225 symmetry error, 0.004571 fractal dimension error, 23.57 worst radius, 25.53 worst texture, 152.5 worst perimeter, 1709.0 worst area, 0.1444 worst smoothness, 0.4245 worst compactness, 0.4504 worst concavity, 0.243 worst concave points, 0.3613 worst symmetry and 0.08758 worst fractal dimension is malignant\n",
      "3          The breast cancer type with 11.42 mean radius, 20.38 mean texture, 77.58 mean perimeter, 386.1 mean area, 0.1425 mean smoothness, 0.2839 mean compactness, 0.2414 mean concavity, 0.1052 mean concave points, 0.2597 mean symmetry, 0.09744 mean fractal dimension, 0.4956 radius error, 1.156 texture error, 3.445 perimeter error, 27.23 area error, 0.00911 smoothness error, 0.07458 compactness error, 0.05661 concavity error, 0.01867 concave points error, 0.05963 symmetry error, 0.009208 fractal dimension error, 14.91 worst radius, 26.5 worst texture, 98.87 worst perimeter, 567.7 worst area, 0.2098 worst smoothness, 0.8663 worst compactness, 0.6869 worst concavity, 0.2575 worst concave points, 0.6638 worst symmetry and 0.173 worst fractal dimension is malignant\n",
      "4         The breast cancer type with 20.29 mean radius, 14.34 mean texture, 135.1 mean perimeter, 1297.0 mean area, 0.1003 mean smoothness, 0.1328 mean compactness, 0.198 mean concavity, 0.1043 mean concave points, 0.1809 mean symmetry, 0.05883 mean fractal dimension, 0.7572 radius error, 0.7813 texture error, 5.438 perimeter error, 94.44 area error, 0.01149 smoothness error, 0.02461 compactness error, 0.05688 concavity error, 0.01885 concave points error, 0.01756 symmetry error, 0.005115 fractal dimension error, 22.54 worst radius, 16.67 worst texture, 152.2 worst perimeter, 1575.0 worst area, 0.1374 worst smoothness, 0.205 worst compactness, 0.4 worst concavity, 0.1625 worst concave points, 0.2364 worst symmetry and 0.07678 worst fractal dimension is malignant\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Autres Exemples\n",
    "\n",
    "display_df(data_to_df_text(datasets.load_digits(), subject_name=\"number\", has_unit=False).head())\n",
    "# display_df(data_to_df_text(datasets.load_diabetes(), subject_name=\"diabetes type\", has_unit=False).head()) # Is regression problem\n",
    "display_df(data_to_df_text(datasets.load_wine(), subject_name=\"wine\", has_unit=False).head())\n",
    "display_df(data_to_df_text(datasets.load_breast_cancer(), subject_name=\"breast cancer type\", has_unit=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Avec modèles classiques et datasets de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n",
      " 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n",
      " 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n",
      " 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n",
      " 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n",
      " 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa' 'setosa'\n",
      " 'setosa' 'setosa' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
      " 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
      " 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
      " 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
      " 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
      " 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
      " 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
      " 'virginica' 'virginica' 'virginica' 'virginica' 'virginica' 'virginica'\n",
      " 'virginica' 'virginica' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# 3. Modèle : DecisionTreeClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "train_clf(clf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                5.1               3.5                1.4               0.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['setosa'], dtype=object)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = X.iloc[0:1]\n",
    "print(q)\n",
    "clf.predict(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question --> When we have sepal length (cm)=0.3, sepal width (cm)=0.4, petal length (cm)=0.1, petal width (cm)=0.2, what should be the iris type ?\n",
      "Réponse --> When we have sepal length (cm)=0.3, sepal width (cm)=0.4, petal length (cm)=0.1, petal width (cm)=0.2, the iris type is setosa\n"
     ]
    }
   ],
   "source": [
    "q = \"When we have sepal length (cm)=0.3, sepal width (cm)=0.4, petal length (cm)=0.1, petal width (cm)=0.2, what should be the iris type ?\"\n",
    "\n",
    "print(\"Question -->\", q)\n",
    "print(\"Réponse -->\", traitement_question(clf, q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Avec modèles de langage pré-entraînés (GPT-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Ecriture dans un fichier dataset.txt\n",
    "# Parce qu'on fait passer un fichier de texte pour le training et non un dataset avec un texte et un label\n",
    "\n",
    "with open('ft_models/iris_dataset.txt', 'w') as file :\n",
    "    for text in iris_text_df :\n",
    "        file.write(text + '.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Directory './ft_models/model2/' created\n",
      "model_name : gpt2\n",
      "dataset_path : ./ft_models/iris_dataset.txt\n",
      "save_path : ./ft_models/model2/fine-tuned-gpt2\n"
     ]
    }
   ],
   "source": [
    "with open('ft_models/counter.txt', 'r') as file :\n",
    "    counter = file.read().strip()\n",
    "print(counter)\n",
    "with open('ft_models/counter.txt', 'w') as file :\n",
    "    file.write(str(int(counter)+1))\n",
    "\n",
    "folder = './ft_models/'\n",
    "folder_model = 'model' + counter + '/'\n",
    "model_name = 'gpt2'\n",
    "dataset_path = 'iris_dataset.txt'\n",
    "save_path = 'fine-tuned-gpt2'\n",
    "\n",
    "path = os.path.join(folder, folder_model) \n",
    "os.mkdir(path) \n",
    "\n",
    "print(\"Directory '% s' created\" % (folder + folder_model)) \n",
    "print(\"model_name : %s\" % model_name)\n",
    "print(\"dataset_path : %s\" % folder + dataset_path)\n",
    "print(\"save_path : %s\" % folder + folder_model + save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Hugging Face GPT-2 Training (mettre un temps pour calculer le temps du training)\n",
    "# 15 min sur le dataset iris\n",
    "\n",
    "train_premodel(\n",
    "    model_name = model_name, \n",
    "    dataset_path = folder + dataset_path, \n",
    "    save_path = folder + folder_model + save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./ft_models/model2/fine-tuned-gpt2\\config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file ./ft_models/model2/fine-tuned-gpt2\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./ft_models/model2/fine-tuned-gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at C:\\Users\\sunse/.cache\\huggingface\\transformers\\684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at C:\\Users\\sunse/.cache\\huggingface\\transformers\\c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at C:\\Users\\sunse/.cache\\huggingface\\transformers\\b105cf342574b32b2f8d5ea86c4845f46d8162160345fd0c85bd9ca3bc5cc48e.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\n",
      "loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at C:\\Users\\sunse/.cache\\huggingface\\transformers\\16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at C:\\Users\\sunse/.cache\\huggingface\\transformers\\fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6.1. Hugging Face GPT-2 Testing\n",
    "\n",
    "model, tokenizer = get_model_tokenizer(\n",
    "    model_path = folder + folder_model + save_path,\n",
    "    tokenizer_path = model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text 1: The flower with 5.4 cm of sepal length, 3.0 cm of sepal width, 1.5 cm of petal length and 1.5 cm of petal width is versicolor.\n",
      "The flower with 5.0 cm of sepal length, 3.0 cm of sepal width, 1.5 cm of petal length and 1.5 cm of petal width is versicolor.\n",
      "The flower with 6.0 cm of sepal length,\n"
     ]
    }
   ],
   "source": [
    "# 6.2. Hugging Face GPT-2 Testing\n",
    "\n",
    "# ---------------------------------\n",
    "# First row : The flower with 5.1 cm of sepal length, 3.5 cm of sepal width, 1.4 cm of petal length and 0.2 cm of petal width is a setosa.\n",
    "# Row 51 : The flower with 7.0 cm of sepal length, 3.2 cm of sepal width, 4.7 cm of petal length and 1.4 cm of petal width is a versicolor.\n",
    "# Row 101 : The flower with 6.3 cm of sepal length, 3.3 cm of sepal width, 6.0 cm of petal length and 2.5 cm of petal width is a virginica.\n",
    "# ---------------------------------\n",
    "\n",
    "test_premodel(model, tokenizer, \"The flower with\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. A suivre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7. Follow Paper protocols\n",
    "\n",
    "# 8. Fix errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
